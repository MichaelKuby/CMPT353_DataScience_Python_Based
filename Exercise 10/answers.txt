Student:    Michael KubyID#:        3015629961.  How long did your reddit_averages.py take with (1) the reddit-0 data set and effectively no work,     (2) no schema specified and not caching (on reddit-2 for this and the rest), (3) with a schema but     not caching, (4) with both a schema and caching the twice-used DataFrame? [The reddit-0 test is     effectively measuring the Spark startup time, so we can see how long it takes to do the actual work    on reddit-2 in the best/worst cases.]            Note: I had trouble getting --master=local[1] to work. It gave me the error:                    zsh: no matches found: --master=local[1]                Instead I ran with                     time spark-submit reddit_averages.py reddit-* reddit-output                This was still able to give me perceptible differences.                (1) the reddit-0 data set and effectively no work:  8.325s          (2) no schema specified and not caching:            15.065s        (3) with schema but not cacheing:                   12.310s        (4) with both a schema and caching:                 14.093            Comments: It looks to me like cacheing was is slowing down the calculation. Based on the discussion        forum, others have had the same experience. Nonetheless, I am submitting with .cache() implemented        because we were told that this is technically the correct thing to do.    2.  Based on the above, does it look like most of the time taken to process the reddit-2 data set is    in reading the files, or calculating the averages?            If cacheing were to have worked correctly, the appropriate response here would have been that most        of the time taken to process the reddit-2 data set is in reading the files, and that calculating the        averages is comparatively less costly.    3.  Where did you use .cache() in your wikipedia_popular.py? [Hint: the answer had better be “once”… but where?]        Since the dataframe with the largest number of page views per hours is constructed from the dataframe        that has filtered out all but English wiki pages, Main_page, and pages starting with 'Special:', it        makes sense to cache immediately after filtering, and immediately before performing the groupBy/aggregate        and join sequence. 
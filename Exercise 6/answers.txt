    Student: Michael Kuby        id#: 3015629961.  In the A/B test analysis, do you feel like we're p-hacking?     How comfortable are you coming to a conclusion at p < 0.05?    I actually think this is precisely what it means to p-hack: you have undertaken a study,realized that what you were looking for could not be confirmed with significance, andthen adjusted the study in such a way to come away with a p-value that is in fact significant.There is a real danger here in that, while it seems like a sincere and well adjustment to make,if we came away once again without significance, are we now at liberty to go ahead and make another adjustment to our analysis that allows us to test again and (hopefully) find significance there? Moreover, we KNOW that based on our alpha of .05, 5% of the time we willfind significance simply by chance, so if we repeat this process enough times, we're likelyto find some significance simply by chance.2.  If we had done T-tests between each pair of sorting implementation results, how many tests     would we run? If we looked for p < 0.05 in them, what would the probability be of having    any false conclusions, just by chance?    It seems to me the formula for the number of tests is given by        f(n) = f(n-1) + (n-1) , for n > 0So for n = 7 we would have run 15 + 6 = 21 tests in total.Given that we would have had to do 21 different t-tests, the probability of no incorrectrejection of the null (given p < 0.05) is 0.95^21 = 0.34 or 34%. That is, we would be morelikely to incorrectly reject the null at some point than to not.3.  Give a ranking of the sorting implementations by speed, including which ones could not     be distinguished. (i.e. which pairs could our experiment not conclude had different running times?)    From best to worst:        1.  Merge, Partition, QS4 and QS5 as a group. In the ANOVA we failed to reject the null between them,        so we cannot say between them which is faster with any certainty.    2.  Qs1 and Qs3 could not be distinguished in the ANOVA; however, QS1 could be distinguished from         QS2 as having a statistically different mean. Additionally, QS3 and QS2 could not themselves be        distinguished as having different means in the ANOVA; hence we can say with some certainty        that QS1 is faster than either QS3 or QS2.    3.  QS3 Cannot be distinguished from QS1 nor QS2; however, QS1 can be distinguished from QS3. Thus,        we could group it with either QS2 or QS1, but it also seems fair to place it between them.    4.  QS2 can be seen to have a statistically different mean from all other sorting algorithms        except for Qs3.                **  Note: Doing multiple runs I noticed that my answers for question 3 could and would change    greatly depending on the size of n. To create my data I chose n = 100 for the number of     elements in each array, ran the array on each sorting algorithm, and repeated this process    for 50 different arrays. So I have 50 different rows of data, each representing a run of    one particular array on each sorting algorithm. **
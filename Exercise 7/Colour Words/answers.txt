1.  Looking at your dog-rates.ipynb, do you think the residuals are close-enough to     being normal to look at the OLS p-value? Can you reasonably conclude that the     ratings are increasing?        Judging by the histogram, it looks as though the residuals are skewed to     the left, calling into question the assumption that the residuals are normally distributed.    Using stats.normaltest(residuals).pvalue I can confirm that, in fact, p < 0.05 meaning    that the residuals are not normally distributed. Thus, we actually cannot confirm    that the slope is different from zero. Intuitively this makes some sonse: When we     see the linear regression fitted against the plotted data, what we see is a compression    of the lower values, not ever-increasing values of y (ratings) as x (time) increases.    I would argue that a more appropriate fit would not actually be a linear regression, but    some sort of curved regression that has an upper bound of somewhere around 14.    2.  Do you think that the new “better” prediction is letting the Kalman filter do a better     job capturing the true signal in the noise?        I suspect that yes, the kalman filter in the second scenario is doing a better job. Why?    Because in exercise 3 we were simply guessing (based on our own intuition) what the    relationship between the next temperature value would be based on the observed values at    the current timestep. Here in exercise 7, we are actually determining these coefficients    by using a linear regression ML model. Further, we are also including more data. And as     prof Baker likes to say, "more data, more better."
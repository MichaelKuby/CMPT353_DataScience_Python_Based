Michael Kuby3015629961.  Based on your results for the last question, do you think daily     temperatures are a good way to predict population density?     Briefly explain why or why not.    Answer:        Not really, no. But it depends in some regard in how you frame the question.    Do I think daily temperatures and population density are linearly     correlated? No, since, if they were, we could plot a line of best fit    on our graph that would start somewhere around (-5, 100) and slope    upwards towards (30, 7000). And, if we extrapolate based on linear correlation,    we might make the absurd argument that the hotter the climate, the more people    that live there. But this is certainly not true, since humans cannot thrive,    much less survive, at certain temperatures.        But it seems like there might be more to this story.    What we do see is that the populations in our data tend to live in     an average max temperature that ranges from around 5 C to 32 C, with    one notable outlier at close to -5 C. And within that range of temperatures,    a range of population densities exist. Were we to plot this on    a histogram and run some statistical tests, we could explore the     distribution and attempt to say something about the likelihood of a     population living inside or outside a given range of avg max temperatures.    For example, it does seems unlikely for extremely dense populations to    live in areas with avg max temperatures outside of -5 C to around 32 C.  2.  Several of the (larger) data files were kept compressed on disk    throughout the analysis, so they needed to be decompressed ever time    we ran our program: that seems inefficient. Why might this be faster     than working with an uncompressed .json or .csv data?        Answer:        I suspect this has something to do with loading the data into memory    precisely when we need it. Were we to work with an uncompressed .json or     .csv data file that contained more information than could be reasonably    stored in memory, some of that information will get moved onto back-storage    while our computer isn't using it. Then, upon running our program, it     may need to make use of swapping, which ultimately slows down execution.        On the other hand, if we leave the file compressed and on disk, as soon as    we require that data, techniques such as on-the-fly decompression and     compression can be used, allowing the reading and writing of small    sections of much larger arrays. In other words, our data will be loaded     into memory (and our caches)in such a way that we optimally access it.